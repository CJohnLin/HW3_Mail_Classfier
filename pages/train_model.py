import streamlit as st
st.set_page_config(page_title='ğŸ§  è¨“ç·´æ¨¡å‹', page_icon='ğŸ§ ')
import streamlit as st
import pandas as pd
import joblib
import os
import time
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
from wordcloud import WordCloud
import matplotlib.pyplot as plt


# =============================
# æ¨™é¡Œèˆ‡èªªæ˜
# =============================
st.markdown("<h1 class='page-title'>ğŸ§  è¨“ç·´åƒåœ¾ç°¡è¨Šåˆ†é¡æ¨¡å‹</h1>", unsafe_allow_html=True)
st.write("ä¸Šå‚³ Datasetï¼ˆéœ€è¦ label, text æ¬„ä½ï¼‰ï¼Œç„¶å¾Œé¸æ“‡æ¨¡å‹å³å¯é–‹å§‹è¨“ç·´ã€‚")

# =============================
# æ–‡å­—æ¸…ç†å‡½å¼
# =============================
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\\S+|www\\S+", "", text)
    text = re.sub(r"\\d+", "", text)
    text = text.translate(str.maketrans("", "", string.punctuation))
    text = re.sub(r"\\s+", " ", text)
    return text.strip()

# =============================
# ä¸Šå‚³ Dataset
# =============================
st.subheader("ğŸ“‚ ä¸Šå‚³ Datasetï¼ˆlabel + textï¼‰")
file = st.file_uploader("é¸æ“‡ CSV æª”æ¡ˆ", type="csv")

st.write("---")

# =============================
# æ¨¡å‹é¸æ“‡å™¨
# =============================
model_type = st.selectbox(
    "é¸æ“‡è¦è¨“ç·´çš„æ¨¡å‹ï¼š",
    ["Logistic Regression", "Naive Bayes", "Linear SVM"]
)

# =============================
# é–‹å§‹è¨“ç·´
# =============================
if st.button("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹", use_container_width=True):

    if file is None:
        st.error("â— è«‹å…ˆä¸Šå‚³ dataset")
        st.stop()

    df = pd.read_csv(file)

    if "label" not in df.columns or "text" not in df.columns:
        st.error("â— Dataset å¿…é ˆåŒ…å« label èˆ‡ text æ¬„ä½")
        st.stop()

    progress = st.progress(0)

    # Step 1 æ¸…ç†è³‡æ–™
    progress.progress(10)
    df["label"] = df["label"].astype(str).str.lower().str.strip()
    df = df[df["label"].isin(["ham", "spam"])]
    df["clean"] = df["text"].astype(str).apply(clean_text)
    df = df[df["clean"] != ""]

    # Step 2 åˆ†å‰²è³‡æ–™
    progress.progress(30)
    X_train, X_test, y_train, y_test = train_test_split(
        df["clean"], df["label"].map({"ham": 0, "spam": 1}),
        test_size=0.2, random_state=42
    )

    # Step 3 TF-IDF
    progress.progress(50)
    vectorizer = TfidfVectorizer(ngram_range=(1, 2))
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    # Step 4 è¨“ç·´æ¨¡å‹
    progress.progress(75)
    if model_type == "Logistic Regression":
        model = LogisticRegression(max_iter=2000)
        model_name = "custom_logreg.joblib"
    elif model_type == "Naive Bayes":
        model = MultinomialNB()
        model_name = "custom_nb.joblib"
    else:
        model = LinearSVC()
        model_name = "custom_svm.joblib"

    model.fit(X_train_vec, y_train)
    preds = model.predict(X_test_vec)

    # Step 5 å„²å­˜æ¨¡å‹
    progress.progress(100)
    os.makedirs("models", exist_ok=True)
    joblib.dump(model, f"models/{model_name}")
    joblib.dump(vectorizer, "models/custom_vectorizer.joblib")

    with open("models/custom_label_map.json", "w") as f:
        f.write('{"0":"ham","1":"spam"}')

    # é¡¯ç¤ºçµæœ
    st.success(f"ğŸ‰ è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²å„²å­˜åˆ° models/{model_name}")
    st.text(classification_report(y_test, preds))

    st.info("ğŸ”„ å³å°‡è‡ªå‹•è¿”å›ã€å–®ç­†åµæ¸¬ã€é é¢â€¦")

    time.sleep(2)
    st.switch_page("message checker")